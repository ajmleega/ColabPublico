{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03zV9levKOJ-"
      },
      "source": [
        "# Trainer Test Run\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AseiSugiyama/TFXTestRun/blob/master/notebooks/TrainerTestRun.ipynb)\n",
        "\n",
        "## Set up\n",
        "\n",
        "TFX requires apache-airflow and docker SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "s8Ug9qPvKOKD",
        "outputId": "f5aaf00a-71d7-4316-de7b-e7ee91aa870c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\r\n",
            "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\r\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.1)\r\n",
            "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\r\n",
            "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\r\n",
            "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\r\n",
            "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\r\n",
            "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\r\n",
            "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\r\n",
            "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\r\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\r\n",
            "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\r\n",
            "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\r\n",
            "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\r\n",
            "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\r\n",
            "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\r\n",
            "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\r\n",
            "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\r\n",
            "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\r\n",
            "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\r\n",
            "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\r\n",
            "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\r\n",
            "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\r\n",
            "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\r\n",
            "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\r\n",
            "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\r\n",
            "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\r\n",
            "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\r\n",
            "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\r\n",
            "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\r\n",
            "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\r\n",
            "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\r\n",
            "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\r\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\r\n",
            "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\r\n",
            "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\r\n",
            "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\r\n",
            "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\r\n",
            "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\r\n",
            "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\r\n",
            "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\r\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\r\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\r\n",
            "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\r\n",
            "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\r\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\r\n",
            "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\r\n",
            "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\r\n",
            "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\r\n",
            "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\r\n",
            "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\r\n",
            "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (41.0.1)\r\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\r\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from tzlocal>=1.4->apache-airflow[gcp]) (2019.1)\r\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\r\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\r\n",
            "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\r\n",
            "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\r\n",
            "Requirement already satisfied: click<8,>=6.7 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (7.0)\r\n",
            "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\r\n",
            "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\r\n",
            "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\r\n",
            "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\r\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\r\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.3.9)\r\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\r\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\r\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\r\n",
            "Requirement already satisfied: WTForms in /usr/local/lib/python3.5/dist-packages (from flask-wtf<0.15,>=0.14.2->apache-airflow[gcp]) (2.2.1)\r\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\r\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\r\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\r\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\r\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.4)\r\n",
            "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\r\n",
            "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\r\n",
            "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.12.0)\r\n",
            "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.2.4)\r\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\r\n",
            "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\r\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\r\n",
            "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\r\n",
            "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\r\n",
            "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\r\n",
            "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\r\n",
            "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\r\n",
            "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\r\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\r\n",
            "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\r\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\r\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\r\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\r\n",
            "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\r\n",
            "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.0)\r\n",
            "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\r\n",
            "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\r\n",
            "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\r\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\r\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\r\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\r\n",
            "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\r\n",
            "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\r\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (4.0)\r\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\r\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.2.5)\r\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\r\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]<3,>=2.12->tfx) (2.4.0)\r\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\r\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.11.1)\r\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\r\n",
            "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\r\n",
            "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\r\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-data-validation<0.14,>=0.13.1->tfx) (1.6.0)\r\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\r\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\r\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\r\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\r\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\r\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\r\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.3.2)\r\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\r\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\r\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\r\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\r\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\r\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\r\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\r\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\r\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\r\n",
            "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\r\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\r\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\r\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.2.0)\r\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\r\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\r\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\r\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\r\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\r\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\r\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\r\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\r\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\r\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\r\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\r\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\r\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\r\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\r\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\r\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\r\n",
            "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\r\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!pip install 'apache-airflow[gcp]' docker tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVeTbsu9KOKF"
      },
      "source": [
        "In this notebook, we use TFX version 0.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLvmG4zcKOKG",
        "outputId": "8840746c-bc2a-4147-a69c-a9ecd55fd9de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.13.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tfx\n",
        "tfx.version.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5q_IA3TKOKG"
      },
      "source": [
        "TFX requires TensorFlow >= 1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgeWS0PVKOKH",
        "outputId": "bad7417e-57c7-4029-dd29-94839efe20c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBEfhQLTKOKH"
      },
      "source": [
        "TFX supports Python 3.5 from version 0.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC3ng42nKOKI",
        "outputId": "f0ff4b19-83d2-4ef8-a8af-cc12d9adccbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40nsWmYmKOKI"
      },
      "source": [
        "## Download sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3zBA089KOKJ",
        "outputId": "ce0c738e-c84b-4fc4-93ea-6a9317f2800c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2019-06-14 05:46:29--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1922668 (1.8M) [text/plain]\n",
            "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  2% 3.74M 0s\n",
            "    50K .......... .......... .......... .......... ..........  5% 3.71M 0s\n",
            "   100K .......... .......... .......... .......... ..........  7% 5.30M 0s\n",
            "   150K .......... .......... .......... .......... .......... 10% 4.59M 0s\n",
            "   200K .......... .......... .......... .......... .......... 13% 3.41M 0s\n",
            "   250K .......... .......... .......... .......... .......... 15% 6.46M 0s\n",
            "   300K .......... .......... .......... .......... .......... 18% 7.98M 0s\n",
            "   350K .......... .......... .......... .......... .......... 21% 6.63M 0s\n",
            "   400K .......... .......... .......... .......... .......... 23% 8.00M 0s\n",
            "   450K .......... .......... .......... .......... .......... 26% 9.95M 0s\n",
            "   500K .......... .......... .......... .......... .......... 29% 8.82M 0s\n",
            "   550K .......... .......... .......... .......... .......... 31% 5.11M 0s\n",
            "   600K .......... .......... .......... .......... .......... 34% 4.13M 0s\n",
            "   650K .......... .......... .......... .......... .......... 37% 25.0M 0s\n",
            "   700K .......... .......... .......... .......... .......... 39% 4.52M 0s\n",
            "   750K .......... .......... .......... .......... .......... 42% 5.70M 0s\n",
            "   800K .......... .......... .......... .......... .......... 45% 11.6M 0s\n",
            "   850K .......... .......... .......... .......... .......... 47% 23.2M 0s\n",
            "   900K .......... .......... .......... .......... .......... 50% 11.9M 0s\n",
            "   950K .......... .......... .......... .......... .......... 53% 7.30M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 55% 11.1M 0s\n",
            "  1050K .......... .......... .......... .......... .......... 58% 13.5M 0s\n",
            "  1100K .......... .......... .......... .......... .......... 61% 20.9M 0s\n",
            "  1150K .......... .......... .......... .......... .......... 63% 7.16M 0s\n",
            "  1200K .......... .......... .......... .......... .......... 66% 25.4M 0s\n",
            "  1250K .......... .......... .......... .......... .......... 69% 6.00M 0s\n",
            "  1300K .......... .......... .......... .......... .......... 71% 26.8M 0s\n",
            "  1350K .......... .......... .......... .......... .......... 74% 4.98M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 77% 5.97M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 79% 20.0M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 82% 7.37M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 85% 18.8M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 87% 9.53M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 90% 21.1M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 93% 8.62M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 95% 17.5M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 98% 24.0M 0s\n",
            "  1850K .......... .......... .......                         100% 4.99M=0.2s\n",
            "\n",
            "2019-06-14 05:46:30 (7.65 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
            "\n",
            "--2019-06-14 05:46:30--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12084 (12K) [text/plain]\n",
            "Saving to: ‘/root/taxi/taxi_utils.py’\n",
            "\n",
            "     0K .......... .                                          100% 10.2M=0.001s\n",
            "\n",
            "2019-06-14 05:46:30 (10.2 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# This enables you to run this notebook twice.\n",
        "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
        "if [ -e ~/taxi/data ]; then\n",
        "    rm -rf ~/taxi/data\n",
        "fi\n",
        "\n",
        "# download taxi data\n",
        "mkdir -p ~/taxi/data/simple\n",
        "mkdir -p ~/taxi/serving_model/taxi_simple\n",
        "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
        "\n",
        "# download \n",
        "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzWXTIKZKOKK"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYZ9EUcxKOKK",
        "outputId": "bb456d8c-995e-4847-cbda-272d94e2a6ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
            "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "from google.protobuf import json_format\n",
        "\n",
        "from tfx.components.base.base_component import ComponentOutputs\n",
        "from tfx.components.evaluator.component import Evaluator\n",
        "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.model_validator.component import ModelValidator\n",
        "from tfx.components.pusher.component import Pusher\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.component import Trainer\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.orchestration.tfx_runner import TfxRunner\n",
        "from tfx.proto import evaluator_pb2\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.utils.dsl_utils import csv_input\n",
        "from tfx.utils.channel import Channel\n",
        "from tfx.utils import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaxefHgOKOKL"
      },
      "source": [
        "## configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlkuLGs2KOKL"
      },
      "outputs": [],
      "source": [
        "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
        "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
        "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
        "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
        "# Python module file to inject customized logic into the TFX components. The\n",
        "# Transform and Trainer both require user-defined functions to run successfully.\n",
        "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
        "\n",
        "# Path which can be listened to by the model server.  Pusher will output the\n",
        "# trained model here.\n",
        "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
        "\n",
        "# Directory and data locations.  This example assumes all of the chicago taxi\n",
        "# example code and metadata library is relative to $HOME, but you can store\n",
        "# these files anywhere on your local filesystem.\n",
        "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
        "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
        "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
        "_log_root = os.path.join(_tfx_root, 'logs')\n",
        "\n",
        "# Airflow-specific configs; these will be passed directly to airflow\n",
        "_airflow_config = {\n",
        "    'schedule_interval': None,\n",
        "    'start_date': datetime.datetime(2019, 1, 1),\n",
        "}\n",
        "\n",
        "# Logging overrides\n",
        "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zTl2XA6KOKM"
      },
      "source": [
        "## Create ExampleGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2MYtIDOKOKM"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
        "examples = csv_input(_data_root)\n",
        "\n",
        "# Brings data into the pipeline or otherwise joins/converts training data.\n",
        "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
        "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
        "output_config = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        train_config,\n",
        "        eval_config\n",
        "    ]))\n",
        "\n",
        "# Create outputs\n",
        "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
        "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
        "\n",
        "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
        "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
        "\n",
        "example_outputs = ComponentOutputs({\n",
        "    'examples': Channel(\n",
        "        type_name='ExamplesPath',\n",
        "        static_artifact_collection=[train_examples, eval_examples]\n",
        "    ),\n",
        "    'training_examples': Channel(\n",
        "        type_name='ExamplesPath',\n",
        "        static_artifact_collection=[train_examples]\n",
        "    ),\n",
        "    'eval_examples': Channel(\n",
        "        type_name='ExamplesPath',\n",
        "        static_artifact_collection=[eval_examples]\n",
        "    ),    \n",
        "})\n",
        "\n",
        "example_gen = CsvExampleGen(\n",
        "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
        "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
        "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257Lx2_OKOKN"
      },
      "source": [
        "## Create StatisticsGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAs3jv3WKOKN"
      },
      "outputs": [],
      "source": [
        "# Create outputs\n",
        "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
        "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
        "\n",
        "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
        "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
        "\n",
        "statistics_outputs = ComponentOutputs({\n",
        "    'output': Channel(\n",
        "        type_name='ExampleStatisticsPath',\n",
        "        static_artifact_collection=[train_statistics, eval_statistics]\n",
        "    )\n",
        "})\n",
        "\n",
        "statistics_gen = StatisticsGen(\n",
        "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
        "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
        "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "empYMQ3-KOKN"
      },
      "source": [
        "## Create SchemaGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SoRQxnKKOKN"
      },
      "outputs": [],
      "source": [
        "# Create outputs\n",
        "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
        "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
        "\n",
        "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
        "# Two or more SchemaPaths will cause ValueError\n",
        "# such as \"ValueError: expected list length of one but got 2\".\n",
        "schema_outputs = ComponentOutputs({\n",
        "    'output':Channel(\n",
        "        type_name='SchemaPath',\n",
        "        static_artifact_collection=[train_schema_path] \n",
        "    )\n",
        "})\n",
        "\n",
        "infer_schema = SchemaGen(\n",
        "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
        "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
        "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wTwb_F8KOKO"
      },
      "source": [
        "## Create Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEthEk3EKOKP"
      },
      "outputs": [],
      "source": [
        "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
        "train_examples.uri = os.path.join(_data_root,\n",
        "                                  'transform/transformed_examples/train/')\n",
        "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
        "eval_examples.uri = os.path.join(_data_root,\n",
        "                                 'transform/transformed_examples/eval/')\n",
        "transform_output = types.TfxType(type_name='TransformPath')\n",
        "transform_output.uri = os.path.join(_data_root,\n",
        "                                    'transform/transform_output/')\n",
        "\n",
        "transform_outputs = ComponentOutputs({\n",
        "    # Output of 'tf.Transform', which includes an exported \n",
        "    # Tensorflow graph suitable for both training and serving\n",
        "    'transform_output':Channel(\n",
        "        type_name='TransformPath',\n",
        "        static_artifact_collection=[transform_output]\n",
        "    ),\n",
        "    # transformed_examples: Materialized transformed examples, which includes \n",
        "    # both 'train' and 'eval' splits.\n",
        "    'transformed_examples':Channel(\n",
        "        type_name='ExamplesPath',\n",
        "        static_artifact_collection=[train_examples, eval_examples]\n",
        "    )\n",
        "})\n",
        "\n",
        "transform = Transform(\n",
        "    input_data=example_gen.outputs.examples,\n",
        "    schema=infer_schema.outputs.output,\n",
        "    module_file=_taxi_module_file,\n",
        "    outputs=transform_outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHHPpG7ZKOKP"
      },
      "source": [
        "## Create Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly-ryCtPKOKP"
      },
      "outputs": [],
      "source": [
        "model_exports = types.TfxType(type_name='ModelExportPath')\n",
        "model_exports.uri = os.path.join(_data_root, 'trainer/current/')\n",
        "\n",
        "trainer_outputs = ComponentOutputs({\n",
        "    'output':Channel(\n",
        "        type_name='ModelExportPath',\n",
        "        static_artifact_collection=[model_exports]\n",
        "    )\n",
        "})\n",
        "\n",
        "trainer = Trainer(\n",
        "    module_file=_taxi_module_file,\n",
        "    transformed_examples=transform.outputs.transformed_examples,\n",
        "    schema=infer_schema.outputs.output,\n",
        "    transform_output=transform.outputs.transform_output,\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
        "    outputs=trainer_outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rjyn80IKOKP"
      },
      "source": [
        "## Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5x5v08cKOKP"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(\n",
        "    pipeline_name=\"TFX Pipeline\",\n",
        "    pipeline_root=_pipeline_root,\n",
        "    components=[example_gen, statistics_gen, infer_schema, transform, trainer]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXwTSN8JKOKQ"
      },
      "source": [
        "## Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Smh8TTzKOKQ"
      },
      "outputs": [],
      "source": [
        "class DirectRunner(TfxRunner):\n",
        "    \"\"\"Tfx runner on local\"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        self._config = config or {}\n",
        "    \n",
        "    def run(self, pipeline):\n",
        "        for component in pipeline.components:\n",
        "            self._execute_component(component)\n",
        "            \n",
        "        return pipeline\n",
        "            \n",
        "    def _execute_component(self, component):\n",
        "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
        "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
        "        exec_properties = component.exec_properties\n",
        "        executor = component.executor()\n",
        "        executor.Do(input_dict, output_dict, exec_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoUlwmvQKOKQ",
        "outputId": "809ef833-ebc7-41b6-8f02-09cb28e64f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting Executor execution.\n",
            "[2019-06-14 06:46:15,380] {base_executor.py:72} INFO - Starting Executor execution.\n",
            "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:46:15,388] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"eval_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"training_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:46:15,393] {base_executor.py:76} INFO - Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"eval_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"training_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
            "[2019-06-14 06:46:15,397] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
            "INFO:tensorflow:Generating examples.\n",
            "[2019-06-14 06:46:15,405] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
            "[2019-06-14 06:46:15,417] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
            "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
            "[2019-06-14 06:46:15,439] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
            "[2019-06-14 06:46:16,248] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f56867a8268> ====================\n",
            "[2019-06-14 06:46:16,250] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f56867a8378> ====================\n",
            "[2019-06-14 06:46:16,253] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f56867a8400> ====================\n",
            "[2019-06-14 06:46:16,255] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f56867a8488> ====================\n",
            "[2019-06-14 06:46:16,257] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f56867a8510> ====================\n",
            "[2019-06-14 06:46:16,260] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f56867a8620> ====================\n",
            "[2019-06-14 06:46:16,262] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f56867a86a8> ====================\n",
            "[2019-06-14 06:46:16,271] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f56867a8730> ====================\n",
            "[2019-06-14 06:46:16,273] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f56867a87b8> ====================\n",
            "[2019-06-14 06:46:16,275] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f56867a8950> ====================\n",
            "[2019-06-14 06:46:16,277] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f56867a89d8> ====================\n",
            "[2019-06-14 06:46:16,279] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f56867a8a60> ====================\n",
            "[2019-06-14 06:46:16,293] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41)+(ref_PCollection_PCollection_25/Write)))+(ref_PCollection_PCollection_24/Write)\n",
            "[2019-06-14 06:46:16,309] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+((((ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8))+(ref_PCollection_PCollection_2/Write))+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write)))\n",
            "[2019-06-14 06:46:17,679] {fn_api_runner.py:437} INFO - Running ((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge))+(((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16))+(ref_PCollection_PCollection_8/Write))\n",
            "[2019-06-14 06:46:17,696] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
            "[2019-06-14 06:46:17,713] {fn_api_runner.py:437} INFO - Running ((((ref_PCollection_PCollection_2/Read)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20))+((ref_AppliedPTransform_InputSourceToExample/ToTFExample_21)+(ref_AppliedPTransform_SerializeDeterministically_22)))+((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+((ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53)+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write)))))+((ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29)+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:23,352] {fn_api_runner.py:437} INFO - Running (ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+(((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35))+(((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44)))+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write)))\n",
            "[2019-06-14 06:46:23,557] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "[2019-06-14 06:46:23,949] {fn_api_runner.py:437} INFO - Running ((OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49))+(ref_PCollection_PCollection_32/Write)\n",
            "[2019-06-14 06:46:23,963] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
            "[2019-06-14 06:46:23,979] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67))+(ref_PCollection_PCollection_42/Write))+(ref_PCollection_PCollection_43/Write)\n",
            "[2019-06-14 06:46:24,002] {fn_api_runner.py:437} INFO - Running ((((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60)+(ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69)))+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70))+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write)\n",
            "[2019-06-14 06:46:24,476] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
            "[2019-06-14 06:46:24,490] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76))+(ref_PCollection_PCollection_51/Write)\n",
            "[2019-06-14 06:46:24,512] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
            "[2019-06-14 06:46:24,527] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:24,633] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2019-06-14 06:46:24,653] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
            "[2019-06-14 06:46:24,663] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:24,771] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "INFO:tensorflow:Examples generated.\n",
            "[2019-06-14 06:46:24,789] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
            "INFO:tensorflow:Starting Executor execution.\n",
            "[2019-06-14 06:46:24,794] {base_executor.py:72} INFO - Starting Executor execution.\n",
            "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:46:24,797] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:46:24,801] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Execution properties for Executor is: {}\n",
            "[2019-06-14 06:46:24,804] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
            "[2019-06-14 06:46:24,812] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
            "INFO:tensorflow:Generating statistics for split eval\n",
            "[2019-06-14 06:46:24,825] {executor.py:62} INFO - Generating statistics for split eval\n",
            "INFO:tensorflow:Generating statistics for split train\n",
            "[2019-06-14 06:46:25,694] {executor.py:62} INFO - Generating statistics for split train\n",
            "INFO:tensorflow:Statistics written to /root/taxi/data/simple/statistics_gen/train/.\n",
            "[2019-06-14 06:46:26,577] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/statistics_gen/train/.\n",
            "[2019-06-14 06:46:29,591] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f56867a8268> ====================\n",
            "[2019-06-14 06:46:29,594] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f56867a8378> ====================\n",
            "[2019-06-14 06:46:29,596] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f56867a8400> ====================\n",
            "[2019-06-14 06:46:29,605] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f56867a8488> ====================\n",
            "[2019-06-14 06:46:29,608] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f56867a8510> ====================\n",
            "[2019-06-14 06:46:29,615] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f56867a8620> ====================\n",
            "[2019-06-14 06:46:29,620] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f56867a86a8> ====================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:29,629] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f56867a8730> ====================\n",
            "[2019-06-14 06:46:29,630] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f56867a87b8> ====================\n",
            "[2019-06-14 06:46:29,633] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f56867a8950> ====================\n",
            "[2019-06-14 06:46:29,636] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f56867a89d8> ====================\n",
            "[2019-06-14 06:46:29,638] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f56867a8a60> ====================\n",
            "[2019-06-14 06:46:29,679] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_ReadData.train/Read_106)+(ref_AppliedPTransform_DecodeData.train/ParseTFExamples_108))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_111))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write))))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))\n",
            "[2019-06-14 06:46:36,191] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadData.eval/Read_3)+(ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_5))+(((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_8)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)))\n",
            "[2019-06-14 06:46:39,750] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+(((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)\n",
            "[2019-06-14 06:46:39,786] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)))\n",
            "[2019-06-14 06:46:39,805] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
            "[2019-06-14 06:46:39,823] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:40,184] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
            "[2019-06-14 06:46:40,393] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
            "[2019-06-14 06:46:40,861] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
            "[2019-06-14 06:46:40,884] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)))\n",
            "[2019-06-14 06:46:40,906] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
            "[2019-06-14 06:46:40,929] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:40,961] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85)+(ref_PCollection_PCollection_51/Write))\n",
            "[2019-06-14 06:46:40,985] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)))+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_97)+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write))\n",
            "[2019-06-14 06:46:41,024] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_102))+(ref_PCollection_PCollection_62/Write)\n",
            "[2019-06-14 06:46:41,044] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
            "[2019-06-14 06:46:41,712] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
            "[2019-06-14 06:46:41,955] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
            "[2019-06-14 06:46:42,282] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+(((((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:42,315] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
            "[2019-06-14 06:46:42,334] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0))\n",
            "[2019-06-14 06:46:42,358] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1))\n",
            "[2019-06-14 06:46:42,376] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
            "[2019-06-14 06:46:42,392] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:42,421] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188))))+(ref_PCollection_PCollection_115/Write)\n",
            "[2019-06-14 06:46:42,441] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191))+((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199))+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_200))+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write))\n",
            "[2019-06-14 06:46:42,476] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_103))+(ref_PCollection_PCollection_63/Write)\n",
            "[2019-06-14 06:46:42,492] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_197)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_198)+(ref_PCollection_PCollection_120/Write)))+(ref_PCollection_PCollection_119/Write)\n",
            "[2019-06-14 06:46:42,512] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_126/Write)\n",
            "[2019-06-14 06:46:42,532] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_206))+(ref_PCollection_PCollection_127/Write)\n",
            "[2019-06-14 06:46:42,553] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_207)\n",
            "[2019-06-14 06:46:42,569] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:42,673] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2019-06-14 06:46:42,698] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_104)\n",
            "[2019-06-14 06:46:42,720] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:42,849] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.12 seconds.\n",
            "INFO:tensorflow:Infering schema from statistics.\n",
            "[2019-06-14 06:46:42,878] {executor.py:62} INFO - Infering schema from statistics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "[2019-06-14 06:46:42,882] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:tensorflow:Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
            "[2019-06-14 06:46:42,900] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
            "INFO:tensorflow:Starting Executor execution.\n",
            "[2019-06-14 06:46:42,904] {base_executor.py:72} INFO - Starting Executor execution.\n",
            "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:42,907] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Outputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:46:42,912] {base_executor.py:76} INFO - Outputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"preprocess.py\"}\n",
            "[2019-06-14 06:46:42,915] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"preprocess.py\"}\n",
            "INFO:tensorflow:Inputs to executor.Transform function: {'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'tft_statistics_use_tfdv': True, 'preprocessing_fn': 'preprocess.py'}\n",
            "[2019-06-14 06:46:42,935] {executor.py:567} INFO - Inputs to executor.Transform function: {'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'tft_statistics_use_tfdv': True, 'preprocessing_fn': 'preprocess.py'}\n",
            "INFO:tensorflow:Outputs to executor.Transform function: {'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path'}\n",
            "[2019-06-14 06:46:42,939] {executor.py:569} INFO - Outputs to executor.Transform function: {'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path'}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[2019-06-14 06:46:43,132] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
            "[2019-06-14 06:46:43,293] {executor.py:653} INFO - Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
            "INFO:tensorflow:Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
            "[2019-06-14 06:46:43,297] {executor.py:655} INFO - Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
            "INFO:tensorflow:Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
            "[2019-06-14 06:46:43,300] {executor.py:657} INFO - Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
            "INFO:tensorflow:Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
            "[2019-06-14 06:46:43,304] {executor.py:658} INFO - Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
            "[2019-06-14 06:46:43,983] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "[2019-06-14 06:46:44,390] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "[2019-06-14 06:46:44,400] {builder_impl.py:654} INFO - Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:44,402] {builder_impl.py:449} INFO - No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/c0516ab5ec7a4f689bf6384c4eff47b6/saved_model.pb\n",
            "[2019-06-14 06:46:44,451] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/c0516ab5ec7a4f689bf6384c4eff47b6/saved_model.pb\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "[2019-06-14 06:46:47,387] {builder_impl.py:654} INFO - Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "[2019-06-14 06:46:47,389] {builder_impl.py:449} INFO - No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/89afd2f5775e44329f1ebb4c700e2dea/saved_model.pb\n",
            "[2019-06-14 06:46:47,430] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/89afd2f5775e44329f1ebb4c700e2dea/saved_model.pb\n",
            "[2019-06-14 06:46:52,489] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f56867a8268> ====================\n",
            "[2019-06-14 06:46:52,497] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f56867a8378> ====================\n",
            "[2019-06-14 06:46:52,505] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f56867a8400> ====================\n",
            "[2019-06-14 06:46:52,515] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f56867a8488> ====================\n",
            "[2019-06-14 06:46:52,520] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f56867a8510> ====================\n",
            "[2019-06-14 06:46:52,531] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f56867a8620> ====================\n",
            "[2019-06-14 06:46:52,537] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f56867a86a8> ====================\n",
            "[2019-06-14 06:46:52,566] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f56867a8730> ====================\n",
            "[2019-06-14 06:46:52,571] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f56867a87b8> ====================\n",
            "[2019-06-14 06:46:52,574] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f56867a8950> ====================\n",
            "[2019-06-14 06:46:52,578] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f56867a89d8> ====================\n",
            "[2019-06-14 06:46:52,581] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f56867a8a60> ====================\n",
            "[2019-06-14 06:46:52,622] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_143)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_144)+(ref_PCollection_PCollection_87/Write)))+(ref_PCollection_PCollection_86/Write)\n",
            "[2019-06-14 06:46:52,642] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_127)+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1))\n",
            "[2019-06-14 06:46:52,659] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModelForAnalyzerInputs[0]/CreateSavedModel/Read_13)+(ref_PCollection_PCollection_6/Write)\n",
            "[2019-06-14 06:46:52,673] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_ReadAnalysisDataset[0]/Read/Read_4)+(ref_AppliedPTransform_ReadAnalysisDataset[0]/AddKey_5))+(ref_AppliedPTransform_ReadAnalysisDataset[0]/ParseExamples_6))+(ref_AppliedPTransform_DecodeAnalysisDataset[0]/ApplyDecodeFn_8))+(FlattenAnalysisDatasets/Write/0)\n",
            "[2019-06-14 06:46:54,412] {fn_api_runner.py:437} INFO - Running ((((((((((FlattenAnalysisDatasets/Read)+((((((((((((((((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/BatchInputs/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_17)+(ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/ApplySavedModel_18))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score/mean_and_var]_19))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_1/mean_and_var]_46)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/KeyWithVoid_49))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_2/mean_and_var]_73))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary/vocabulary]_100))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1/vocabulary]_158))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize/quantiles]_216)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/KeyWithVoid_219)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_1/quantiles]_248))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_2/quantiles]_280))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_3/quantiles]_312))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/KeyWithVoid_315)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/KeyWithVoid_76))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FlattenStringsAndMaybeWeightsLabels_102)))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FlattenStringsAndMaybeWeightsLabels_160))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CountPerString:PairWithVoid_162))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/KeyWithVoid_283)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/KeyWithVoid_22)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CountPerString:PairWithVoid_104)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/KeyWithVoid_251)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:54,862] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:55,945] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FilterProblematicStrings_112)))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Write))\n",
            "[2019-06-14 06:46:55,968] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Merge))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapStringsAndCounts_121)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_125))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)\n",
            "[2019-06-14 06:46:55,994] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
            "[2019-06-14 06:46:56,007] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_133))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_134)+(ref_PCollection_PCollection_83/Write))\n",
            "[2019-06-14 06:46:56,031] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Read_137)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_138))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_145)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_146)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)))\n",
            "[2019-06-14 06:46:56,065] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_151))+(ref_PCollection_PCollection_93/Write)\n",
            "[2019-06-14 06:46:56,082] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_152)+(ref_PCollection_PCollection_94/Write))\n",
            "[2019-06-14 06:46:56,105] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/UnKey_57)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_60)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:56,138] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/UnKey_68)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_70)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder]_71))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/8)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder_1]_72)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/9)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/9)))))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/8)\n",
            "[2019-06-14 06:46:56,181] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_201)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_202))+(ref_PCollection_PCollection_122/Write))+(ref_PCollection_PCollection_123/Write)\n",
            "[2019-06-14 06:46:56,202] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_185)+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1))\n",
            "[2019-06-14 06:46:56,221] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FilterProblematicStrings_170)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Precombine))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Write))\n",
            "[2019-06-14 06:46:56,251] {fn_api_runner.py:437} INFO - Running (((((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Merge))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapStringsAndCounts_179))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_183))+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:56,282] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
            "[2019-06-14 06:46:56,297] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_191))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_192)+(ref_PCollection_PCollection_119/Write))\n",
            "[2019-06-14 06:46:56,327] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Read_195)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_196))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_203))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_204)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write))\n",
            "[2019-06-14 06:46:56,365] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_209))+(ref_PCollection_PCollection_129/Write)\n",
            "[2019-06-14 06:46:56,381] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_122/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_210))+(ref_PCollection_PCollection_130/Write)\n",
            "[2019-06-14 06:46:56,399] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_122/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_211))+(ref_PCollection_PCollection_131/Write)\n",
            "[2019-06-14 06:46:56,415] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:56,519] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2019-06-14 06:46:56,542] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Read_213)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_214))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]_215)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/1))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/1))\n",
            "[2019-06-14 06:46:56,574] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_153)+(ref_PCollection_PCollection_95/Write))\n",
            "[2019-06-14 06:46:56,595] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:46:56,710] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
            "[2019-06-14 06:46:56,724] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Read_155)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_156))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]_157))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/0))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/0)\n",
            "[2019-06-14 06:46:56,755] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/UnKey_259)))+(ref_PCollection_PCollection_161/Write)\n",
            "[2019-06-14 06:46:56,933] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/DoOnce/Read_261)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/InjectDefault_262))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/KeyWithVoid_265)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:57,040] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/UnKey_273))+(ref_PCollection_PCollection_169/Write)))\n",
            "[2019-06-14 06:46:57,221] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/DoOnce/Read_275)+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/InjectDefault_276)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_278)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_1/quantiles/Placeholder]_279)))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/3))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/3))\n",
            "[2019-06-14 06:46:57,253] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/UnKey_227)))+(ref_PCollection_PCollection_141/Write)\n",
            "[2019-06-14 06:46:57,425] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/DoOnce/Read_229)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/InjectDefault_230))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_233)+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
            "[2019-06-14 06:46:57,526] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_241))+(ref_PCollection_PCollection_149/Write)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:57,679] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Read_243)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_244)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_246)))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize/quantiles/Placeholder]_247))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/2))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/2)\n",
            "[2019-06-14 06:46:57,711] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/UnKey_84)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_87))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:57,741] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/UnKey_95))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_97)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder]_98)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/10)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/10))))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder_1]_99)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/11)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/11)))\n",
            "[2019-06-14 06:46:57,788] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/UnKey_30)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_33)+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
            "[2019-06-14 06:46:57,817] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/UnKey_41)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_43)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder]_44))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/6)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/6))))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder_1]_45)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/7))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/7))\n",
            "[2019-06-14 06:46:57,878] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/UnKey_291))+(ref_PCollection_PCollection_181/Write)\n",
            "[2019-06-14 06:46:58,052] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/DoOnce/Read_293)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/InjectDefault_294)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/KeyWithVoid_297)))+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
            "[2019-06-14 06:46:58,146] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/UnKey_305)+(ref_PCollection_PCollection_189/Write))))\n",
            "[2019-06-14 06:46:58,585] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/DoOnce/Read_307)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/InjectDefault_308)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_310)))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_2/quantiles/Placeholder]_311)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/4))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/4))\n",
            "[2019-06-14 06:46:58,619] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/UnKey_323)))+(ref_PCollection_PCollection_201/Write))\n",
            "[2019-06-14 06:46:58,840] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/DoOnce/Read_325)+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/InjectDefault_326)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/KeyWithVoid_329))+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:46:58,979] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/UnKey_337)+(ref_PCollection_PCollection_209/Write)))\n",
            "[2019-06-14 06:46:59,158] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/DoOnce/Read_339)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/InjectDefault_340))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_342)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_3/quantiles/Placeholder]_343)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/5)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/5))\n",
            "[2019-06-14 06:46:59,200] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CreateSavedModel/Flatten/Read)+(ref_PCollection_PCollection_216/Write)\n",
            "[2019-06-14 06:46:59,226] {fn_api_runner.py:437} INFO - Running ((((((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/CreateSavedModel/Read_346)+(ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/BindTensors_348))+(ref_AppliedPTransform_AnalyzeDataset/ComputeDeferredMetadata_349))+(ref_AppliedPTransform_AnalyzeDataset/MakeCheapBarrier_350))+(ref_AppliedPTransform_WriteTransformFn/WriteTransformFn_361))+(ref_PCollection_PCollection_217/Write))+(ref_PCollection_PCollection_219/Write))+(ref_PCollection_PCollection_225/Write))+((ref_AppliedPTransform_WriteTransformFn/WriteMetadata/WriteMetadata_360)+(ref_PCollection_PCollection_224/Write))\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:59,352] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "[2019-06-14 06:46:59,383] {builder_impl.py:654} INFO - Assets added to graph.\n",
            "INFO:tensorflow:Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/041f55bc4dbf481bbdee06ac266ca147/assets\n",
            "[2019-06-14 06:46:59,389] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/041f55bc4dbf481bbdee06ac266ca147/assets\n",
            "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/041f55bc4dbf481bbdee06ac266ca147/saved_model.pb\n",
            "[2019-06-14 06:46:59,432] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/041f55bc4dbf481bbdee06ac266ca147/saved_model.pb\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:46:59,521] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:46:59,525] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:59,532] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:59,658] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/DoOnce/Read_405)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/InitializeWrite_406))+(ref_PCollection_PCollection_250/Write))+(ref_PCollection_PCollection_251/Write)\n",
            "[2019-06-14 06:46:59,691] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ReadTransformDataset[0]/Read/Read_365)+(ref_AppliedPTransform_ReadTransformDataset[0]/AddKey_366))+(((((ref_AppliedPTransform_ReadTransformDataset[0]/ParseExamples_367)+(ref_AppliedPTransform_DecodeTransformDataset[0]/ApplyDecodeFn_369))+((ref_AppliedPTransform_TransformDataset[0]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_373)+(ref_AppliedPTransform_TransformDataset[0]/Transform_374)))+((((ref_AppliedPTransform_TransformDataset[0]/ConvertAndUnbatch_375)+(ref_AppliedPTransform_TransformDataset[0]/MakeCheapBarrier_376))+(ref_AppliedPTransform_EncodeTransformedDataset[0]_380))+(ref_PCollection_PCollection_234/Write)))+(((ref_AppliedPTransform_Materialize[0]/DropNoneKeys_400)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WriteBundles_407))+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Pair_408)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_409)+(Materialize[0]/Write/Write/WriteImpl/GroupByKey/Write)))))\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:46:59,858] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:46:59,861] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:46:59,863] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:47:04,132] {fn_api_runner.py:437} INFO - Running (Materialize[0]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Extract_414)+(ref_PCollection_PCollection_258/Write))\n",
            "[2019-06-14 06:47:04,150] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/DoOnce/Read_423)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/InitializeWrite_424))+(ref_PCollection_PCollection_262/Write))+(ref_PCollection_PCollection_263/Write)\n",
            "[2019-06-14 06:47:04,179] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_WriteMetadata/Create/Read_356)+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_357)\n",
            "[2019-06-14 06:47:04,204] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadTransformDataset[1]/Read/Read_383)+(((ref_AppliedPTransform_ReadTransformDataset[1]/AddKey_384)+(ref_AppliedPTransform_ReadTransformDataset[1]/ParseExamples_385))+(ref_AppliedPTransform_DecodeTransformDataset[1]/ApplyDecodeFn_387)))+(ref_AppliedPTransform_TransformDataset[1]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_391))+((ref_AppliedPTransform_TransformDataset[1]/Transform_392)+(((((((ref_AppliedPTransform_TransformDataset[1]/ConvertAndUnbatch_393)+((ref_AppliedPTransform_TransformDataset[1]/MakeCheapBarrier_394)+(ref_PCollection_PCollection_245/Write)))+(ref_AppliedPTransform_EncodeTransformedDataset[1]_398))+(ref_AppliedPTransform_Materialize[1]/DropNoneKeys_418))+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WriteBundles_425))+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Pair_426)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_427)))+(Materialize[1]/Write/Write/WriteImpl/GroupByKey/Write)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:47:04,561] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:47:04,564] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:47:04,567] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:47:07,021] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[1]/PrepareToClearSharedKeepAlives/Read_396)+(ref_AppliedPTransform_TransformDataset[1]/WaitAndClearSharedKeepAlives_397)\n",
            "[2019-06-14 06:47:07,045] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/PreFinalize_415))+(ref_PCollection_PCollection_259/Write)\n",
            "[2019-06-14 06:47:07,068] {fn_api_runner.py:437} INFO - Running ((Materialize[1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Extract_432))+(ref_PCollection_PCollection_270/Write)\n",
            "[2019-06-14 06:47:07,087] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/PreFinalize_433))+(ref_PCollection_PCollection_271/Write)\n",
            "[2019-06-14 06:47:07,111] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/FinalizeWrite_434)\n",
            "[2019-06-14 06:47:07,128] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:47:07,232] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2019-06-14 06:47:07,259] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[0]/PrepareToClearSharedKeepAlives/Read_378)+(ref_AppliedPTransform_TransformDataset[0]/WaitAndClearSharedKeepAlives_379)\n",
            "[2019-06-14 06:47:07,288] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_225/Read)+(ref_AppliedPTransform_WriteTransformFn/WaitOnWriteMetadataDone_362)\n",
            "[2019-06-14 06:47:07,318] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/PrepareToClearSharedKeepAlives/Read_352)+(ref_AppliedPTransform_AnalyzeDataset/WaitAndClearSharedKeepAlives_353)\n",
            "[2019-06-14 06:47:07,345] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/FinalizeWrite_416)\n",
            "[2019-06-14 06:47:07,365] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2019-06-14 06:47:07,469] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "INFO:tensorflow:Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
            "[2019-06-14 06:47:07,499] {executor.py:248} INFO - Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
            "INFO:tensorflow:Starting Executor execution.\n",
            "[2019-06-14 06:47:07,515] {base_executor.py:72} INFO - Starting Executor execution.\n",
            "INFO:tensorflow:Inputs for Executor is: {\"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:47:07,518] {base_executor.py:74} INFO - Inputs for Executor is: {\"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}], \"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"state\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "[2019-06-14 06:47:07,522] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"span\": \"INT\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
            "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"custom_config\": null, \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\"}\n",
            "[2019-06-14 06:47:07,529] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"custom_config\": null, \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\"}\n",
            "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 1, '_service': None, '_log_step_count_steps': 100, '_device_fn': None, '_train_distribute': None, '_master': '', '_tf_random_seed': None, '_num_ps_replicas': 0, '_task_id': 0, '_save_checkpoints_secs': None, '_eval_distribute': None, '_is_chief': True, '_num_worker_replicas': 1, '_experimental_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56386ffe80>, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_global_id_in_cluster': 0, '_save_checkpoints_steps': 999, '_protocol': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_evaluation_master': '', '_task_type': 'worker', '_save_summary_steps': 100}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:47:07,548] {estimator.py:201} INFO - Using config: {'_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 1, '_service': None, '_log_step_count_steps': 100, '_device_fn': None, '_train_distribute': None, '_master': '', '_tf_random_seed': None, '_num_ps_replicas': 0, '_task_id': 0, '_save_checkpoints_secs': None, '_eval_distribute': None, '_is_chief': True, '_num_worker_replicas': 1, '_experimental_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56386ffe80>, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_global_id_in_cluster': 0, '_save_checkpoints_steps': 999, '_protocol': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_evaluation_master': '', '_task_type': 'worker', '_save_summary_steps': 100}\n",
            "INFO:tensorflow:Training model.\n",
            "[2019-06-14 06:47:07,552] {executor.py:141} INFO - Training model.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "[2019-06-14 06:47:07,555] {estimator_training.py:185} INFO - Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "[2019-06-14 06:47:07,558] {training.py:610} INFO - Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
            "[2019-06-14 06:47:07,561] {training.py:698} INFO - Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "[2019-06-14 06:47:07,577] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "[2019-06-14 06:47:07,706] {estimator.py:1111} INFO - Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[2019-06-14 06:47:07,724] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "[2019-06-14 06:47:10,091] {estimator.py:1113} INFO - Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "[2019-06-14 06:47:10,094] {basic_session_run_hooks.py:527} INFO - Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "[2019-06-14 06:47:11,054] {monitored_session.py:222} INFO - Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "[2019-06-14 06:47:11,244] {session_manager.py:491} INFO - Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[2019-06-14 06:47:11,283] {session_manager.py:493} INFO - Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:12,404] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:loss = 27.75816, step = 1\n",
            "[2019-06-14 06:47:13,702] {basic_session_run_hooks.py:249} INFO - loss = 27.75816, step = 1\n",
            "INFO:tensorflow:global_step/sec: 155.586\n",
            "[2019-06-14 06:47:14,344] {basic_session_run_hooks.py:680} INFO - global_step/sec: 155.586\n",
            "INFO:tensorflow:loss = 20.733616, step = 101 (0.645 sec)\n",
            "[2019-06-14 06:47:14,347] {basic_session_run_hooks.py:247} INFO - loss = 20.733616, step = 101 (0.645 sec)\n",
            "INFO:tensorflow:global_step/sec: 323.641\n",
            "[2019-06-14 06:47:14,653] {basic_session_run_hooks.py:680} INFO - global_step/sec: 323.641\n",
            "INFO:tensorflow:loss = 17.224764, step = 201 (0.312 sec)\n",
            "[2019-06-14 06:47:14,659] {basic_session_run_hooks.py:247} INFO - loss = 17.224764, step = 201 (0.312 sec)\n",
            "INFO:tensorflow:global_step/sec: 255.584\n",
            "[2019-06-14 06:47:15,044] {basic_session_run_hooks.py:680} INFO - global_step/sec: 255.584\n",
            "INFO:tensorflow:loss = 20.39127, step = 301 (0.393 sec)\n",
            "[2019-06-14 06:47:15,052] {basic_session_run_hooks.py:247} INFO - loss = 20.39127, step = 301 (0.393 sec)\n",
            "INFO:tensorflow:global_step/sec: 276.348\n",
            "[2019-06-14 06:47:15,406] {basic_session_run_hooks.py:680} INFO - global_step/sec: 276.348\n",
            "INFO:tensorflow:loss = 19.01436, step = 401 (0.362 sec)\n",
            "[2019-06-14 06:47:15,414] {basic_session_run_hooks.py:247} INFO - loss = 19.01436, step = 401 (0.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 250.479\n",
            "[2019-06-14 06:47:15,805] {basic_session_run_hooks.py:680} INFO - global_step/sec: 250.479\n",
            "INFO:tensorflow:loss = 17.909664, step = 501 (0.396 sec)\n",
            "[2019-06-14 06:47:15,811] {basic_session_run_hooks.py:247} INFO - loss = 17.909664, step = 501 (0.396 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.872\n",
            "[2019-06-14 06:47:16,063] {basic_session_run_hooks.py:680} INFO - global_step/sec: 386.872\n",
            "INFO:tensorflow:loss = 14.584776, step = 601 (0.260 sec)\n",
            "[2019-06-14 06:47:16,071] {basic_session_run_hooks.py:247} INFO - loss = 14.584776, step = 601 (0.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 276.696\n",
            "[2019-06-14 06:47:16,425] {basic_session_run_hooks.py:680} INFO - global_step/sec: 276.696\n",
            "INFO:tensorflow:loss = 18.771374, step = 701 (0.360 sec)\n",
            "[2019-06-14 06:47:16,431] {basic_session_run_hooks.py:247} INFO - loss = 18.771374, step = 701 (0.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 237.504\n",
            "[2019-06-14 06:47:16,846] {basic_session_run_hooks.py:680} INFO - global_step/sec: 237.504\n",
            "INFO:tensorflow:loss = 16.367464, step = 801 (0.419 sec)\n",
            "[2019-06-14 06:47:16,850] {basic_session_run_hooks.py:247} INFO - loss = 16.367464, step = 801 (0.419 sec)\n",
            "INFO:tensorflow:global_step/sec: 262.356\n",
            "[2019-06-14 06:47:17,227] {basic_session_run_hooks.py:680} INFO - global_step/sec: 262.356\n",
            "INFO:tensorflow:loss = 18.948353, step = 901 (0.382 sec)\n",
            "[2019-06-14 06:47:17,232] {basic_session_run_hooks.py:247} INFO - loss = 18.948353, step = 901 (0.382 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:17,592] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[2019-06-14 06:47:17,604] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "[2019-06-14 06:47:17,966] {estimator.py:1111} INFO - Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "[2019-06-14 06:47:19,826] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:47:20,200] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:47:20,240] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "[2019-06-14 06:47:20,277] {estimator.py:1113} INFO - Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-14T06:47:20Z\n",
            "[2019-06-14 06:47:20,310] {evaluation.py:257} INFO - Starting evaluation at 2019-06-14T06:47:20Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "[2019-06-14 06:47:20,520] {monitored_session.py:222} INFO - Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "[2019-06-14 06:47:20,522] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
            "[2019-06-14 06:47:20,528] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "[2019-06-14 06:47:20,640] {session_manager.py:491} INFO - Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[2019-06-14 06:47:20,684] {session_manager.py:493} INFO - Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [500/5000]\n",
            "[2019-06-14 06:47:23,775] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
            "INFO:tensorflow:Evaluation [1000/5000]\n",
            "[2019-06-14 06:47:25,693] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
            "INFO:tensorflow:Evaluation [1500/5000]\n",
            "[2019-06-14 06:47:27,397] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
            "INFO:tensorflow:Evaluation [2000/5000]\n",
            "[2019-06-14 06:47:28,872] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
            "INFO:tensorflow:Evaluation [2500/5000]\n",
            "[2019-06-14 06:47:30,587] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
            "INFO:tensorflow:Evaluation [3000/5000]\n",
            "[2019-06-14 06:47:32,192] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
            "INFO:tensorflow:Evaluation [3500/5000]\n",
            "[2019-06-14 06:47:34,204] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
            "INFO:tensorflow:Evaluation [4000/5000]\n",
            "[2019-06-14 06:47:35,902] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
            "INFO:tensorflow:Evaluation [4500/5000]\n",
            "[2019-06-14 06:47:37,408] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
            "INFO:tensorflow:Evaluation [5000/5000]\n",
            "[2019-06-14 06:47:39,056] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-14-06:47:39\n",
            "[2019-06-14 06:47:39,154] {evaluation.py:277} INFO - Finished evaluation at 2019-06-14-06:47:39\n",
            "INFO:tensorflow:Saving dict for global step 999: accuracy = 0.769715, accuracy_baseline = 0.769715, auc = 0.9030457, auc_precision_recall = 0.6423779, average_loss = 0.4528628, global_step = 999, label/mean = 0.230285, loss = 18.114513, precision = 0.0, prediction/mean = 0.23077416, recall = 0.0\n",
            "[2019-06-14 06:47:39,156] {estimator.py:1979} INFO - Saving dict for global step 999: accuracy = 0.769715, accuracy_baseline = 0.769715, auc = 0.9030457, auc_precision_recall = 0.6423779, average_loss = 0.4528628, global_step = 999, label/mean = 0.230285, loss = 18.114513, precision = 0.0, prediction/mean = 0.23077416, recall = 0.0\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
            "[2019-06-14 06:47:39,433] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
            "INFO:tensorflow:global_step/sec: 4.49995\n",
            "[2019-06-14 06:47:39,450] {basic_session_run_hooks.py:680} INFO - global_step/sec: 4.49995\n",
            "INFO:tensorflow:loss = 18.073914, step = 1001 (22.224 sec)\n",
            "[2019-06-14 06:47:39,456] {basic_session_run_hooks.py:247} INFO - loss = 18.073914, step = 1001 (22.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.222\n",
            "[2019-06-14 06:47:39,707] {basic_session_run_hooks.py:680} INFO - global_step/sec: 388.222\n",
            "INFO:tensorflow:loss = 17.03189, step = 1101 (0.256 sec)\n",
            "[2019-06-14 06:47:39,712] {basic_session_run_hooks.py:247} INFO - loss = 17.03189, step = 1101 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 269.372\n",
            "[2019-06-14 06:47:40,078] {basic_session_run_hooks.py:680} INFO - global_step/sec: 269.372\n",
            "INFO:tensorflow:loss = 16.584513, step = 1201 (0.375 sec)\n",
            "[2019-06-14 06:47:40,087] {basic_session_run_hooks.py:247} INFO - loss = 16.584513, step = 1201 (0.375 sec)\n",
            "INFO:tensorflow:global_step/sec: 261.2\n",
            "[2019-06-14 06:47:40,461] {basic_session_run_hooks.py:680} INFO - global_step/sec: 261.2\n",
            "INFO:tensorflow:loss = 17.154575, step = 1301 (0.379 sec)\n",
            "[2019-06-14 06:47:40,466] {basic_session_run_hooks.py:247} INFO - loss = 17.154575, step = 1301 (0.379 sec)\n",
            "INFO:tensorflow:global_step/sec: 248.225\n",
            "[2019-06-14 06:47:40,864] {basic_session_run_hooks.py:680} INFO - global_step/sec: 248.225\n",
            "INFO:tensorflow:loss = 16.508648, step = 1401 (0.405 sec)\n",
            "[2019-06-14 06:47:40,871] {basic_session_run_hooks.py:247} INFO - loss = 16.508648, step = 1401 (0.405 sec)\n",
            "INFO:tensorflow:global_step/sec: 342.829\n",
            "[2019-06-14 06:47:41,156] {basic_session_run_hooks.py:680} INFO - global_step/sec: 342.829\n",
            "INFO:tensorflow:loss = 18.598427, step = 1501 (0.292 sec)\n",
            "[2019-06-14 06:47:41,163] {basic_session_run_hooks.py:247} INFO - loss = 18.598427, step = 1501 (0.292 sec)\n",
            "INFO:tensorflow:global_step/sec: 287.181\n",
            "[2019-06-14 06:47:41,504] {basic_session_run_hooks.py:680} INFO - global_step/sec: 287.181\n",
            "INFO:tensorflow:loss = 15.205364, step = 1601 (0.344 sec)\n",
            "[2019-06-14 06:47:41,507] {basic_session_run_hooks.py:247} INFO - loss = 15.205364, step = 1601 (0.344 sec)\n",
            "INFO:tensorflow:global_step/sec: 268.517\n",
            "[2019-06-14 06:47:41,877] {basic_session_run_hooks.py:680} INFO - global_step/sec: 268.517\n",
            "INFO:tensorflow:loss = 16.622875, step = 1701 (0.378 sec)\n",
            "[2019-06-14 06:47:41,885] {basic_session_run_hooks.py:247} INFO - loss = 16.622875, step = 1701 (0.378 sec)\n",
            "INFO:tensorflow:global_step/sec: 295.95\n",
            "[2019-06-14 06:47:42,214] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.95\n",
            "INFO:tensorflow:loss = 17.012978, step = 1801 (0.336 sec)\n",
            "[2019-06-14 06:47:42,220] {basic_session_run_hooks.py:247} INFO - loss = 17.012978, step = 1801 (0.336 sec)\n",
            "INFO:tensorflow:global_step/sec: 261.883\n",
            "[2019-06-14 06:47:42,596] {basic_session_run_hooks.py:680} INFO - global_step/sec: 261.883\n",
            "INFO:tensorflow:loss = 20.217232, step = 1901 (0.380 sec)\n",
            "[2019-06-14 06:47:42,600] {basic_session_run_hooks.py:247} INFO - loss = 20.217232, step = 1901 (0.380 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:42,986] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:47:43,240] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 151.064\n",
            "[2019-06-14 06:47:43,258] {basic_session_run_hooks.py:680} INFO - global_step/sec: 151.064\n",
            "INFO:tensorflow:loss = 15.612, step = 2001 (0.663 sec)\n",
            "[2019-06-14 06:47:43,263] {basic_session_run_hooks.py:247} INFO - loss = 15.612, step = 2001 (0.663 sec)\n",
            "INFO:tensorflow:global_step/sec: 259.081\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:47:43,644] {basic_session_run_hooks.py:680} INFO - global_step/sec: 259.081\n",
            "INFO:tensorflow:loss = 16.93203, step = 2101 (0.388 sec)\n",
            "[2019-06-14 06:47:43,651] {basic_session_run_hooks.py:247} INFO - loss = 16.93203, step = 2101 (0.388 sec)\n",
            "INFO:tensorflow:global_step/sec: 267.33\n",
            "[2019-06-14 06:47:44,018] {basic_session_run_hooks.py:680} INFO - global_step/sec: 267.33\n",
            "INFO:tensorflow:loss = 13.191904, step = 2201 (0.371 sec)\n",
            "[2019-06-14 06:47:44,022] {basic_session_run_hooks.py:247} INFO - loss = 13.191904, step = 2201 (0.371 sec)\n",
            "INFO:tensorflow:global_step/sec: 350.815\n",
            "[2019-06-14 06:47:44,303] {basic_session_run_hooks.py:680} INFO - global_step/sec: 350.815\n",
            "INFO:tensorflow:loss = 20.152155, step = 2301 (0.288 sec)\n",
            "[2019-06-14 06:47:44,310] {basic_session_run_hooks.py:247} INFO - loss = 20.152155, step = 2301 (0.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 301.039\n",
            "[2019-06-14 06:47:44,635] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.039\n",
            "INFO:tensorflow:loss = 15.477424, step = 2401 (0.329 sec)\n",
            "[2019-06-14 06:47:44,639] {basic_session_run_hooks.py:247} INFO - loss = 15.477424, step = 2401 (0.329 sec)\n",
            "INFO:tensorflow:global_step/sec: 260.146\n",
            "[2019-06-14 06:47:45,020] {basic_session_run_hooks.py:680} INFO - global_step/sec: 260.146\n",
            "INFO:tensorflow:loss = 15.831751, step = 2501 (0.388 sec)\n",
            "[2019-06-14 06:47:45,027] {basic_session_run_hooks.py:247} INFO - loss = 15.831751, step = 2501 (0.388 sec)\n",
            "INFO:tensorflow:global_step/sec: 226.419\n",
            "[2019-06-14 06:47:45,461] {basic_session_run_hooks.py:680} INFO - global_step/sec: 226.419\n",
            "INFO:tensorflow:loss = 15.204399, step = 2601 (0.450 sec)\n",
            "[2019-06-14 06:47:45,478] {basic_session_run_hooks.py:247} INFO - loss = 15.204399, step = 2601 (0.450 sec)\n",
            "INFO:tensorflow:global_step/sec: 318.658\n",
            "[2019-06-14 06:47:45,775] {basic_session_run_hooks.py:680} INFO - global_step/sec: 318.658\n",
            "INFO:tensorflow:loss = 15.009147, step = 2701 (0.303 sec)\n",
            "[2019-06-14 06:47:45,781] {basic_session_run_hooks.py:247} INFO - loss = 15.009147, step = 2701 (0.303 sec)\n",
            "INFO:tensorflow:global_step/sec: 329.313\n",
            "[2019-06-14 06:47:46,079] {basic_session_run_hooks.py:680} INFO - global_step/sec: 329.313\n",
            "INFO:tensorflow:loss = 15.021156, step = 2801 (0.303 sec)\n",
            "[2019-06-14 06:47:46,084] {basic_session_run_hooks.py:247} INFO - loss = 15.021156, step = 2801 (0.303 sec)\n",
            "INFO:tensorflow:global_step/sec: 273.57\n",
            "[2019-06-14 06:47:46,444] {basic_session_run_hooks.py:680} INFO - global_step/sec: 273.57\n",
            "INFO:tensorflow:loss = 18.154612, step = 2901 (0.365 sec)\n",
            "[2019-06-14 06:47:46,449] {basic_session_run_hooks.py:247} INFO - loss = 18.154612, step = 2901 (0.365 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:46,814] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:47:47,068] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 153.407\n",
            "[2019-06-14 06:47:47,096] {basic_session_run_hooks.py:680} INFO - global_step/sec: 153.407\n",
            "INFO:tensorflow:loss = 16.27833, step = 3001 (0.653 sec)\n",
            "[2019-06-14 06:47:47,102] {basic_session_run_hooks.py:247} INFO - loss = 16.27833, step = 3001 (0.653 sec)\n",
            "INFO:tensorflow:global_step/sec: 269.76\n",
            "[2019-06-14 06:47:47,467] {basic_session_run_hooks.py:680} INFO - global_step/sec: 269.76\n",
            "INFO:tensorflow:loss = 15.240412, step = 3101 (0.375 sec)\n",
            "[2019-06-14 06:47:47,477] {basic_session_run_hooks.py:247} INFO - loss = 15.240412, step = 3101 (0.375 sec)\n",
            "INFO:tensorflow:global_step/sec: 282.664\n",
            "[2019-06-14 06:47:47,821] {basic_session_run_hooks.py:680} INFO - global_step/sec: 282.664\n",
            "INFO:tensorflow:loss = 17.93284, step = 3201 (0.355 sec)\n",
            "[2019-06-14 06:47:47,832] {basic_session_run_hooks.py:247} INFO - loss = 17.93284, step = 3201 (0.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 268.61\n",
            "[2019-06-14 06:47:48,193] {basic_session_run_hooks.py:680} INFO - global_step/sec: 268.61\n",
            "INFO:tensorflow:loss = 16.089859, step = 3301 (0.365 sec)\n",
            "[2019-06-14 06:47:48,197] {basic_session_run_hooks.py:247} INFO - loss = 16.089859, step = 3301 (0.365 sec)\n",
            "INFO:tensorflow:global_step/sec: 301.868\n",
            "[2019-06-14 06:47:48,524] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.868\n",
            "INFO:tensorflow:loss = 13.836031, step = 3401 (0.332 sec)\n",
            "[2019-06-14 06:47:48,528] {basic_session_run_hooks.py:247} INFO - loss = 13.836031, step = 3401 (0.332 sec)\n",
            "INFO:tensorflow:global_step/sec: 282.802\n",
            "[2019-06-14 06:47:48,878] {basic_session_run_hooks.py:680} INFO - global_step/sec: 282.802\n",
            "INFO:tensorflow:loss = 12.720139, step = 3501 (0.354 sec)\n",
            "[2019-06-14 06:47:48,883] {basic_session_run_hooks.py:247} INFO - loss = 12.720139, step = 3501 (0.354 sec)\n",
            "INFO:tensorflow:global_step/sec: 301.48\n",
            "[2019-06-14 06:47:49,210] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.48\n",
            "INFO:tensorflow:loss = 15.011563, step = 3601 (0.331 sec)\n",
            "[2019-06-14 06:47:49,214] {basic_session_run_hooks.py:247} INFO - loss = 15.011563, step = 3601 (0.331 sec)\n",
            "INFO:tensorflow:global_step/sec: 276.228\n",
            "[2019-06-14 06:47:49,572] {basic_session_run_hooks.py:680} INFO - global_step/sec: 276.228\n",
            "INFO:tensorflow:loss = 14.797858, step = 3701 (0.362 sec)\n",
            "[2019-06-14 06:47:49,576] {basic_session_run_hooks.py:247} INFO - loss = 14.797858, step = 3701 (0.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 385.608\n",
            "[2019-06-14 06:47:49,831] {basic_session_run_hooks.py:680} INFO - global_step/sec: 385.608\n",
            "INFO:tensorflow:loss = 19.07977, step = 3801 (0.260 sec)\n",
            "[2019-06-14 06:47:49,836] {basic_session_run_hooks.py:247} INFO - loss = 19.07977, step = 3801 (0.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 361.206\n",
            "[2019-06-14 06:47:50,108] {basic_session_run_hooks.py:680} INFO - global_step/sec: 361.206\n",
            "INFO:tensorflow:loss = 16.465504, step = 3901 (0.276 sec)\n",
            "[2019-06-14 06:47:50,111] {basic_session_run_hooks.py:247} INFO - loss = 16.465504, step = 3901 (0.276 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:50,429] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:47:50,630] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 179.849\n",
            "[2019-06-14 06:47:50,664] {basic_session_run_hooks.py:680} INFO - global_step/sec: 179.849\n",
            "INFO:tensorflow:loss = 18.104454, step = 4001 (0.558 sec)\n",
            "[2019-06-14 06:47:50,669] {basic_session_run_hooks.py:247} INFO - loss = 18.104454, step = 4001 (0.558 sec)\n",
            "INFO:tensorflow:global_step/sec: 348.47\n",
            "[2019-06-14 06:47:50,951] {basic_session_run_hooks.py:680} INFO - global_step/sec: 348.47\n",
            "INFO:tensorflow:loss = 11.798531, step = 4101 (0.288 sec)\n",
            "[2019-06-14 06:47:50,957] {basic_session_run_hooks.py:247} INFO - loss = 11.798531, step = 4101 (0.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.676\n",
            "[2019-06-14 06:47:51,209] {basic_session_run_hooks.py:680} INFO - global_step/sec: 386.676\n",
            "INFO:tensorflow:loss = 13.696853, step = 4201 (0.260 sec)\n",
            "[2019-06-14 06:47:51,218] {basic_session_run_hooks.py:247} INFO - loss = 13.696853, step = 4201 (0.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.886\n",
            "[2019-06-14 06:47:51,458] {basic_session_run_hooks.py:680} INFO - global_step/sec: 402.886\n",
            "INFO:tensorflow:loss = 14.098616, step = 4301 (0.244 sec)\n",
            "[2019-06-14 06:47:51,462] {basic_session_run_hooks.py:247} INFO - loss = 14.098616, step = 4301 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.445\n",
            "[2019-06-14 06:47:51,715] {basic_session_run_hooks.py:680} INFO - global_step/sec: 388.445\n",
            "INFO:tensorflow:loss = 16.929745, step = 4401 (0.259 sec)\n",
            "[2019-06-14 06:47:51,721] {basic_session_run_hooks.py:247} INFO - loss = 16.929745, step = 4401 (0.259 sec)\n",
            "INFO:tensorflow:global_step/sec: 362.95\n",
            "[2019-06-14 06:47:51,991] {basic_session_run_hooks.py:680} INFO - global_step/sec: 362.95\n",
            "INFO:tensorflow:loss = 15.120426, step = 4501 (0.275 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:47:51,996] {basic_session_run_hooks.py:247} INFO - loss = 15.120426, step = 4501 (0.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.979\n",
            "[2019-06-14 06:47:52,231] {basic_session_run_hooks.py:680} INFO - global_step/sec: 414.979\n",
            "INFO:tensorflow:loss = 15.069384, step = 4601 (0.242 sec)\n",
            "[2019-06-14 06:47:52,239] {basic_session_run_hooks.py:247} INFO - loss = 15.069384, step = 4601 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 393.449\n",
            "[2019-06-14 06:47:52,486] {basic_session_run_hooks.py:680} INFO - global_step/sec: 393.449\n",
            "INFO:tensorflow:loss = 17.666027, step = 4701 (0.253 sec)\n",
            "[2019-06-14 06:47:52,492] {basic_session_run_hooks.py:247} INFO - loss = 17.666027, step = 4701 (0.253 sec)\n",
            "INFO:tensorflow:global_step/sec: 378.466\n",
            "[2019-06-14 06:47:52,750] {basic_session_run_hooks.py:680} INFO - global_step/sec: 378.466\n",
            "INFO:tensorflow:loss = 14.762946, step = 4801 (0.264 sec)\n",
            "[2019-06-14 06:47:52,756] {basic_session_run_hooks.py:247} INFO - loss = 14.762946, step = 4801 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 367.964\n",
            "[2019-06-14 06:47:53,022] {basic_session_run_hooks.py:680} INFO - global_step/sec: 367.964\n",
            "INFO:tensorflow:loss = 16.120522, step = 4901 (0.270 sec)\n",
            "[2019-06-14 06:47:53,025] {basic_session_run_hooks.py:247} INFO - loss = 16.120522, step = 4901 (0.270 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:53,272] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:47:53,467] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 210.352\n",
            "[2019-06-14 06:47:53,497] {basic_session_run_hooks.py:680} INFO - global_step/sec: 210.352\n",
            "INFO:tensorflow:loss = 14.663102, step = 5001 (0.479 sec)\n",
            "[2019-06-14 06:47:53,504] {basic_session_run_hooks.py:247} INFO - loss = 14.663102, step = 5001 (0.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 308.784\n",
            "[2019-06-14 06:47:53,821] {basic_session_run_hooks.py:680} INFO - global_step/sec: 308.784\n",
            "INFO:tensorflow:loss = 18.917603, step = 5101 (0.322 sec)\n",
            "[2019-06-14 06:47:53,827] {basic_session_run_hooks.py:247} INFO - loss = 18.917603, step = 5101 (0.322 sec)\n",
            "INFO:tensorflow:global_step/sec: 341.944\n",
            "[2019-06-14 06:47:54,113] {basic_session_run_hooks.py:680} INFO - global_step/sec: 341.944\n",
            "INFO:tensorflow:loss = 13.493209, step = 5201 (0.290 sec)\n",
            "[2019-06-14 06:47:54,117] {basic_session_run_hooks.py:247} INFO - loss = 13.493209, step = 5201 (0.290 sec)\n",
            "INFO:tensorflow:global_step/sec: 353.862\n",
            "[2019-06-14 06:47:54,396] {basic_session_run_hooks.py:680} INFO - global_step/sec: 353.862\n",
            "INFO:tensorflow:loss = 16.410776, step = 5301 (0.285 sec)\n",
            "[2019-06-14 06:47:54,402] {basic_session_run_hooks.py:247} INFO - loss = 16.410776, step = 5301 (0.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 351.952\n",
            "[2019-06-14 06:47:54,680] {basic_session_run_hooks.py:680} INFO - global_step/sec: 351.952\n",
            "INFO:tensorflow:loss = 13.666452, step = 5401 (0.282 sec)\n",
            "[2019-06-14 06:47:54,684] {basic_session_run_hooks.py:247} INFO - loss = 13.666452, step = 5401 (0.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 345.159\n",
            "[2019-06-14 06:47:54,970] {basic_session_run_hooks.py:680} INFO - global_step/sec: 345.159\n",
            "INFO:tensorflow:loss = 16.441986, step = 5501 (0.293 sec)\n",
            "[2019-06-14 06:47:54,977] {basic_session_run_hooks.py:247} INFO - loss = 16.441986, step = 5501 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 336.448\n",
            "[2019-06-14 06:47:55,267] {basic_session_run_hooks.py:680} INFO - global_step/sec: 336.448\n",
            "INFO:tensorflow:loss = 13.749449, step = 5601 (0.299 sec)\n",
            "[2019-06-14 06:47:55,276] {basic_session_run_hooks.py:247} INFO - loss = 13.749449, step = 5601 (0.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 254.921\n",
            "[2019-06-14 06:47:55,659] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.921\n",
            "INFO:tensorflow:loss = 11.884867, step = 5701 (0.394 sec)\n",
            "[2019-06-14 06:47:55,670] {basic_session_run_hooks.py:247} INFO - loss = 11.884867, step = 5701 (0.394 sec)\n",
            "INFO:tensorflow:global_step/sec: 280.957\n",
            "[2019-06-14 06:47:56,015] {basic_session_run_hooks.py:680} INFO - global_step/sec: 280.957\n",
            "INFO:tensorflow:loss = 14.619041, step = 5801 (0.352 sec)\n",
            "[2019-06-14 06:47:56,022] {basic_session_run_hooks.py:247} INFO - loss = 14.619041, step = 5801 (0.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 263.232\n",
            "[2019-06-14 06:47:56,395] {basic_session_run_hooks.py:680} INFO - global_step/sec: 263.232\n",
            "INFO:tensorflow:loss = 13.395766, step = 5901 (0.378 sec)\n",
            "[2019-06-14 06:47:56,400] {basic_session_run_hooks.py:247} INFO - loss = 13.395766, step = 5901 (0.378 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:47:56,749] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:47:56,949] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 169.848\n",
            "[2019-06-14 06:47:56,984] {basic_session_run_hooks.py:680} INFO - global_step/sec: 169.848\n",
            "INFO:tensorflow:loss = 14.730801, step = 6001 (0.591 sec)\n",
            "[2019-06-14 06:47:56,991] {basic_session_run_hooks.py:247} INFO - loss = 14.730801, step = 6001 (0.591 sec)\n",
            "INFO:tensorflow:global_step/sec: 313.974\n",
            "[2019-06-14 06:47:57,302] {basic_session_run_hooks.py:680} INFO - global_step/sec: 313.974\n",
            "INFO:tensorflow:loss = 15.149296, step = 6101 (0.319 sec)\n",
            "[2019-06-14 06:47:57,310] {basic_session_run_hooks.py:247} INFO - loss = 15.149296, step = 6101 (0.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 291.992\n",
            "[2019-06-14 06:47:57,645] {basic_session_run_hooks.py:680} INFO - global_step/sec: 291.992\n",
            "INFO:tensorflow:loss = 14.520562, step = 6201 (0.341 sec)\n",
            "[2019-06-14 06:47:57,651] {basic_session_run_hooks.py:247} INFO - loss = 14.520562, step = 6201 (0.341 sec)\n",
            "INFO:tensorflow:global_step/sec: 292.27\n",
            "[2019-06-14 06:47:57,987] {basic_session_run_hooks.py:680} INFO - global_step/sec: 292.27\n",
            "INFO:tensorflow:loss = 13.240284, step = 6301 (0.346 sec)\n",
            "[2019-06-14 06:47:57,997] {basic_session_run_hooks.py:247} INFO - loss = 13.240284, step = 6301 (0.346 sec)\n",
            "INFO:tensorflow:global_step/sec: 260.903\n",
            "[2019-06-14 06:47:58,370] {basic_session_run_hooks.py:680} INFO - global_step/sec: 260.903\n",
            "INFO:tensorflow:loss = 15.0186825, step = 6401 (0.383 sec)\n",
            "[2019-06-14 06:47:58,379] {basic_session_run_hooks.py:247} INFO - loss = 15.0186825, step = 6401 (0.383 sec)\n",
            "INFO:tensorflow:global_step/sec: 337.991\n",
            "[2019-06-14 06:47:58,666] {basic_session_run_hooks.py:680} INFO - global_step/sec: 337.991\n",
            "INFO:tensorflow:loss = 11.910047, step = 6501 (0.291 sec)\n",
            "[2019-06-14 06:47:58,670] {basic_session_run_hooks.py:247} INFO - loss = 11.910047, step = 6501 (0.291 sec)\n",
            "INFO:tensorflow:global_step/sec: 228.772\n",
            "[2019-06-14 06:47:59,103] {basic_session_run_hooks.py:680} INFO - global_step/sec: 228.772\n",
            "INFO:tensorflow:loss = 14.035671, step = 6601 (0.438 sec)\n",
            "[2019-06-14 06:47:59,108] {basic_session_run_hooks.py:247} INFO - loss = 14.035671, step = 6601 (0.438 sec)\n",
            "INFO:tensorflow:global_step/sec: 341.91\n",
            "[2019-06-14 06:47:59,396] {basic_session_run_hooks.py:680} INFO - global_step/sec: 341.91\n",
            "INFO:tensorflow:loss = 13.601072, step = 6701 (0.295 sec)\n",
            "[2019-06-14 06:47:59,403] {basic_session_run_hooks.py:247} INFO - loss = 13.601072, step = 6701 (0.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 280.371\n",
            "[2019-06-14 06:47:59,753] {basic_session_run_hooks.py:680} INFO - global_step/sec: 280.371\n",
            "INFO:tensorflow:loss = 11.6604, step = 6801 (0.359 sec)\n",
            "[2019-06-14 06:47:59,762] {basic_session_run_hooks.py:247} INFO - loss = 11.6604, step = 6801 (0.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 283.223\n",
            "[2019-06-14 06:48:00,106] {basic_session_run_hooks.py:680} INFO - global_step/sec: 283.223\n",
            "INFO:tensorflow:loss = 16.106836, step = 6901 (0.353 sec)\n",
            "[2019-06-14 06:48:00,114] {basic_session_run_hooks.py:247} INFO - loss = 16.106836, step = 6901 (0.353 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:48:00,434] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:48:00,743] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 145.701\n",
            "[2019-06-14 06:48:00,792] {basic_session_run_hooks.py:680} INFO - global_step/sec: 145.701\n",
            "INFO:tensorflow:loss = 11.043283, step = 7001 (0.684 sec)\n",
            "[2019-06-14 06:48:00,798] {basic_session_run_hooks.py:247} INFO - loss = 11.043283, step = 7001 (0.684 sec)\n",
            "INFO:tensorflow:global_step/sec: 241.005\n",
            "[2019-06-14 06:48:01,207] {basic_session_run_hooks.py:680} INFO - global_step/sec: 241.005\n",
            "INFO:tensorflow:loss = 16.511175, step = 7101 (0.413 sec)\n",
            "[2019-06-14 06:48:01,212] {basic_session_run_hooks.py:247} INFO - loss = 16.511175, step = 7101 (0.413 sec)\n",
            "INFO:tensorflow:global_step/sec: 383.658\n",
            "[2019-06-14 06:48:01,467] {basic_session_run_hooks.py:680} INFO - global_step/sec: 383.658\n",
            "INFO:tensorflow:loss = 13.851409, step = 7201 (0.261 sec)\n",
            "[2019-06-14 06:48:01,473] {basic_session_run_hooks.py:247} INFO - loss = 13.851409, step = 7201 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 371.449\n",
            "[2019-06-14 06:48:01,737] {basic_session_run_hooks.py:680} INFO - global_step/sec: 371.449\n",
            "INFO:tensorflow:loss = 15.6963825, step = 7301 (0.277 sec)\n",
            "[2019-06-14 06:48:01,750] {basic_session_run_hooks.py:247} INFO - loss = 15.6963825, step = 7301 (0.277 sec)\n",
            "INFO:tensorflow:global_step/sec: 280.5\n",
            "[2019-06-14 06:48:02,093] {basic_session_run_hooks.py:680} INFO - global_step/sec: 280.5\n",
            "INFO:tensorflow:loss = 18.586697, step = 7401 (0.349 sec)\n",
            "[2019-06-14 06:48:02,100] {basic_session_run_hooks.py:247} INFO - loss = 18.586697, step = 7401 (0.349 sec)\n",
            "INFO:tensorflow:global_step/sec: 261.181\n",
            "[2019-06-14 06:48:02,476] {basic_session_run_hooks.py:680} INFO - global_step/sec: 261.181\n",
            "INFO:tensorflow:loss = 10.96279, step = 7501 (0.383 sec)\n",
            "[2019-06-14 06:48:02,482] {basic_session_run_hooks.py:247} INFO - loss = 10.96279, step = 7501 (0.383 sec)\n",
            "INFO:tensorflow:global_step/sec: 224.307\n",
            "[2019-06-14 06:48:02,922] {basic_session_run_hooks.py:680} INFO - global_step/sec: 224.307\n",
            "INFO:tensorflow:loss = 13.209331, step = 7601 (0.444 sec)\n",
            "[2019-06-14 06:48:02,926] {basic_session_run_hooks.py:247} INFO - loss = 13.209331, step = 7601 (0.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 315.506\n",
            "[2019-06-14 06:48:03,239] {basic_session_run_hooks.py:680} INFO - global_step/sec: 315.506\n",
            "INFO:tensorflow:loss = 14.658016, step = 7701 (0.318 sec)\n",
            "[2019-06-14 06:48:03,244] {basic_session_run_hooks.py:247} INFO - loss = 14.658016, step = 7701 (0.318 sec)\n",
            "INFO:tensorflow:global_step/sec: 318.691\n",
            "[2019-06-14 06:48:03,553] {basic_session_run_hooks.py:680} INFO - global_step/sec: 318.691\n",
            "INFO:tensorflow:loss = 15.972006, step = 7801 (0.317 sec)\n",
            "[2019-06-14 06:48:03,561] {basic_session_run_hooks.py:247} INFO - loss = 15.972006, step = 7801 (0.317 sec)\n",
            "INFO:tensorflow:global_step/sec: 261.05\n",
            "[2019-06-14 06:48:03,936] {basic_session_run_hooks.py:680} INFO - global_step/sec: 261.05\n",
            "INFO:tensorflow:loss = 14.844985, step = 7901 (0.385 sec)\n",
            "[2019-06-14 06:48:03,945] {basic_session_run_hooks.py:247} INFO - loss = 14.844985, step = 7901 (0.385 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:48:04,364] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:48:04,651] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 130.62\n",
            "[2019-06-14 06:48:04,701] {basic_session_run_hooks.py:680} INFO - global_step/sec: 130.62\n",
            "INFO:tensorflow:loss = 11.448263, step = 8001 (0.763 sec)\n",
            "[2019-06-14 06:48:04,709] {basic_session_run_hooks.py:247} INFO - loss = 11.448263, step = 8001 (0.763 sec)\n",
            "INFO:tensorflow:global_step/sec: 279.019\n",
            "[2019-06-14 06:48:05,060] {basic_session_run_hooks.py:680} INFO - global_step/sec: 279.019\n",
            "INFO:tensorflow:loss = 18.226627, step = 8101 (0.356 sec)\n",
            "[2019-06-14 06:48:05,065] {basic_session_run_hooks.py:247} INFO - loss = 18.226627, step = 8101 (0.356 sec)\n",
            "INFO:tensorflow:global_step/sec: 245.882\n",
            "[2019-06-14 06:48:05,466] {basic_session_run_hooks.py:680} INFO - global_step/sec: 245.882\n",
            "INFO:tensorflow:loss = 14.127532, step = 8201 (0.408 sec)\n",
            "[2019-06-14 06:48:05,472] {basic_session_run_hooks.py:247} INFO - loss = 14.127532, step = 8201 (0.408 sec)\n",
            "INFO:tensorflow:global_step/sec: 248.421\n",
            "[2019-06-14 06:48:05,869] {basic_session_run_hooks.py:680} INFO - global_step/sec: 248.421\n",
            "INFO:tensorflow:loss = 14.237951, step = 8301 (0.404 sec)\n",
            "[2019-06-14 06:48:05,876] {basic_session_run_hooks.py:247} INFO - loss = 14.237951, step = 8301 (0.404 sec)\n",
            "INFO:tensorflow:global_step/sec: 234.658\n",
            "[2019-06-14 06:48:06,295] {basic_session_run_hooks.py:680} INFO - global_step/sec: 234.658\n",
            "INFO:tensorflow:loss = 10.62936, step = 8401 (0.423 sec)\n",
            "[2019-06-14 06:48:06,300] {basic_session_run_hooks.py:247} INFO - loss = 10.62936, step = 8401 (0.423 sec)\n",
            "INFO:tensorflow:global_step/sec: 254.51\n",
            "[2019-06-14 06:48:06,688] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.51\n",
            "INFO:tensorflow:loss = 12.341303, step = 8501 (0.393 sec)\n",
            "[2019-06-14 06:48:06,692] {basic_session_run_hooks.py:247} INFO - loss = 12.341303, step = 8501 (0.393 sec)\n",
            "INFO:tensorflow:global_step/sec: 259.333\n",
            "[2019-06-14 06:48:07,074] {basic_session_run_hooks.py:680} INFO - global_step/sec: 259.333\n",
            "INFO:tensorflow:loss = 10.0099745, step = 8601 (0.390 sec)\n",
            "[2019-06-14 06:48:07,082] {basic_session_run_hooks.py:247} INFO - loss = 10.0099745, step = 8601 (0.390 sec)\n",
            "INFO:tensorflow:global_step/sec: 227.015\n",
            "[2019-06-14 06:48:07,514] {basic_session_run_hooks.py:680} INFO - global_step/sec: 227.015\n",
            "INFO:tensorflow:loss = 11.926129, step = 8701 (0.438 sec)\n",
            "[2019-06-14 06:48:07,520] {basic_session_run_hooks.py:247} INFO - loss = 11.926129, step = 8701 (0.438 sec)\n",
            "INFO:tensorflow:global_step/sec: 295.22\n",
            "[2019-06-14 06:48:07,853] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.22\n",
            "INFO:tensorflow:loss = 12.179042, step = 8801 (0.339 sec)\n",
            "[2019-06-14 06:48:07,859] {basic_session_run_hooks.py:247} INFO - loss = 12.179042, step = 8801 (0.339 sec)\n",
            "INFO:tensorflow:global_step/sec: 330.28\n",
            "[2019-06-14 06:48:08,156] {basic_session_run_hooks.py:680} INFO - global_step/sec: 330.28\n",
            "INFO:tensorflow:loss = 8.826737, step = 8901 (0.302 sec)\n",
            "[2019-06-14 06:48:08,161] {basic_session_run_hooks.py:247} INFO - loss = 8.826737, step = 8901 (0.302 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:48:08,389] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:48:08,590] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 195.595\n",
            "[2019-06-14 06:48:08,667] {basic_session_run_hooks.py:680} INFO - global_step/sec: 195.595\n",
            "INFO:tensorflow:loss = 11.450289, step = 9001 (0.522 sec)\n",
            "[2019-06-14 06:48:08,683] {basic_session_run_hooks.py:247} INFO - loss = 11.450289, step = 9001 (0.522 sec)\n",
            "INFO:tensorflow:global_step/sec: 197.795\n",
            "[2019-06-14 06:48:09,172] {basic_session_run_hooks.py:680} INFO - global_step/sec: 197.795\n",
            "INFO:tensorflow:loss = 16.623882, step = 9101 (0.495 sec)\n",
            "[2019-06-14 06:48:09,178] {basic_session_run_hooks.py:247} INFO - loss = 16.623882, step = 9101 (0.495 sec)\n",
            "INFO:tensorflow:global_step/sec: 325.368\n",
            "[2019-06-14 06:48:09,480] {basic_session_run_hooks.py:680} INFO - global_step/sec: 325.368\n",
            "INFO:tensorflow:loss = 13.062143, step = 9201 (0.310 sec)\n",
            "[2019-06-14 06:48:09,488] {basic_session_run_hooks.py:247} INFO - loss = 13.062143, step = 9201 (0.310 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 250.393\n",
            "[2019-06-14 06:48:09,879] {basic_session_run_hooks.py:680} INFO - global_step/sec: 250.393\n",
            "INFO:tensorflow:loss = 13.946808, step = 9301 (0.396 sec)\n",
            "[2019-06-14 06:48:09,884] {basic_session_run_hooks.py:247} INFO - loss = 13.946808, step = 9301 (0.396 sec)\n",
            "INFO:tensorflow:global_step/sec: 314.214\n",
            "[2019-06-14 06:48:10,197] {basic_session_run_hooks.py:680} INFO - global_step/sec: 314.214\n",
            "INFO:tensorflow:loss = 14.231247, step = 9401 (0.319 sec)\n",
            "[2019-06-14 06:48:10,202] {basic_session_run_hooks.py:247} INFO - loss = 14.231247, step = 9401 (0.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 105.02\n",
            "[2019-06-14 06:48:11,150] {basic_session_run_hooks.py:680} INFO - global_step/sec: 105.02\n",
            "INFO:tensorflow:loss = 13.067789, step = 9501 (0.951 sec)\n",
            "[2019-06-14 06:48:11,154] {basic_session_run_hooks.py:247} INFO - loss = 13.067789, step = 9501 (0.951 sec)\n",
            "INFO:tensorflow:global_step/sec: 309.122\n",
            "[2019-06-14 06:48:11,473] {basic_session_run_hooks.py:680} INFO - global_step/sec: 309.122\n",
            "INFO:tensorflow:loss = 14.30411, step = 9601 (0.325 sec)\n",
            "[2019-06-14 06:48:11,478] {basic_session_run_hooks.py:247} INFO - loss = 14.30411, step = 9601 (0.325 sec)\n",
            "INFO:tensorflow:global_step/sec: 244.571\n",
            "[2019-06-14 06:48:11,882] {basic_session_run_hooks.py:680} INFO - global_step/sec: 244.571\n",
            "INFO:tensorflow:loss = 14.034039, step = 9701 (0.407 sec)\n",
            "[2019-06-14 06:48:11,885] {basic_session_run_hooks.py:247} INFO - loss = 14.034039, step = 9701 (0.407 sec)\n",
            "INFO:tensorflow:global_step/sec: 254.71\n",
            "[2019-06-14 06:48:12,274] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.71\n",
            "INFO:tensorflow:loss = 14.320034, step = 9801 (0.393 sec)\n",
            "[2019-06-14 06:48:12,278] {basic_session_run_hooks.py:247} INFO - loss = 14.320034, step = 9801 (0.393 sec)\n",
            "INFO:tensorflow:global_step/sec: 228.629\n",
            "[2019-06-14 06:48:12,712] {basic_session_run_hooks.py:680} INFO - global_step/sec: 228.629\n",
            "INFO:tensorflow:loss = 15.67024, step = 9901 (0.439 sec)\n",
            "[2019-06-14 06:48:12,717] {basic_session_run_hooks.py:247} INFO - loss = 15.67024, step = 9901 (0.439 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:48:13,029] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:48:13,384] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "[2019-06-14 06:48:13,424] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "[2019-06-14 06:48:13,745] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "[2019-06-14 06:48:13,863] {estimator.py:1111} INFO - Calling model_fn.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:48:15,887] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:48:15,924] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "[2019-06-14 06:48:15,957] {estimator.py:1113} INFO - Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-14T06:48:15Z\n",
            "[2019-06-14 06:48:16,004] {evaluation.py:257} INFO - Starting evaluation at 2019-06-14T06:48:15Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "[2019-06-14 06:48:16,566] {monitored_session.py:222} INFO - Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "[2019-06-14 06:48:16,573] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "[2019-06-14 06:48:16,748] {session_manager.py:491} INFO - Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[2019-06-14 06:48:16,805] {session_manager.py:493} INFO - Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [500/5000]\n",
            "[2019-06-14 06:48:19,753] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
            "INFO:tensorflow:Evaluation [1000/5000]\n",
            "[2019-06-14 06:48:21,410] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
            "INFO:tensorflow:Evaluation [1500/5000]\n",
            "[2019-06-14 06:48:22,842] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
            "INFO:tensorflow:Evaluation [2000/5000]\n",
            "[2019-06-14 06:48:24,294] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
            "INFO:tensorflow:Evaluation [2500/5000]\n",
            "[2019-06-14 06:48:26,004] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
            "INFO:tensorflow:Evaluation [3000/5000]\n",
            "[2019-06-14 06:48:27,540] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
            "INFO:tensorflow:Evaluation [3500/5000]\n",
            "[2019-06-14 06:48:29,693] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
            "INFO:tensorflow:Evaluation [4000/5000]\n",
            "[2019-06-14 06:48:31,314] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
            "INFO:tensorflow:Evaluation [4500/5000]\n",
            "[2019-06-14 06:48:32,846] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
            "INFO:tensorflow:Evaluation [5000/5000]\n",
            "[2019-06-14 06:48:35,123] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-14-06:48:35\n",
            "[2019-06-14 06:48:35,275] {evaluation.py:277} INFO - Finished evaluation at 2019-06-14-06:48:35\n",
            "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.795575, accuracy_baseline = 0.76968, auc = 0.94485617, auc_precision_recall = 0.73576236, average_loss = 0.3360544, global_step = 10000, label/mean = 0.23032, loss = 13.442177, precision = 0.7259007, prediction/mean = 0.22783701, recall = 0.18063998\n",
            "[2019-06-14 06:48:35,280] {estimator.py:1979} INFO - Saving dict for global step 10000: accuracy = 0.795575, accuracy_baseline = 0.76968, auc = 0.94485617, auc_precision_recall = 0.73576236, average_loss = 0.3360544, global_step = 10000, label/mean = 0.23032, loss = 13.442177, precision = 0.7259007, prediction/mean = 0.22783701, recall = 0.18063998\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "[2019-06-14 06:48:35,287] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n",
            "[2019-06-14 06:48:35,292] {exporter.py:415} INFO - Performing the final export in the end of training.\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:48:35,428] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:48:35,430] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:48:35,433] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2019-06-14 06:48:35,441] {estimator.py:1111} INFO - Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "[2019-06-14 06:48:36,740] {estimator.py:1113} INFO - Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
            "[2019-06-14 06:48:36,744] {export.py:587} INFO - Signatures INCLUDED in export for Predict: ['predict']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
            "[2019-06-14 06:48:36,748] {export.py:587} INFO - Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
            "[2019-06-14 06:48:36,750] {export.py:587} INFO - Signatures INCLUDED in export for Regress: ['regression']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "[2019-06-14 06:48:36,752] {export.py:587} INFO - Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "[2019-06-14 06:48:36,755] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "[2019-06-14 06:48:37,262] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "[2019-06-14 06:48:37,342] {builder_impl.py:654} INFO - Assets added to graph.\n",
            "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1560494915'/assets\n",
            "[2019-06-14 06:48:37,346] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1560494915'/assets\n",
            "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1560494915'/saved_model.pb\n",
            "[2019-06-14 06:48:37,675] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1560494915'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 17.258242.\n",
            "[2019-06-14 06:48:37,728] {estimator.py:359} INFO - Loss for final step: 17.258242.\n",
            "INFO:tensorflow:Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
            "[2019-06-14 06:48:37,734] {executor.py:146} INFO - Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
            "INFO:tensorflow:Exporting eval_savedmodel for TFMA.\n",
            "[2019-06-14 06:48:37,738] {executor.py:149} INFO - Exporting eval_savedmodel for TFMA.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
            "Instructions for updating:\n",
            "Use estimator.experimental_export_all_saved_models\n",
            "[2019-06-14 06:48:37,741] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
            "Instructions for updating:\n",
            "Use estimator.experimental_export_all_saved_models\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:48:37,850] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_2:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "[2019-06-14 06:48:37,853] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "[2019-06-14 06:48:37,856] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "[2019-06-14 06:48:37,915] {estimator.py:1111} INFO - Calling model_fn.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:48:40,281] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "[2019-06-14 06:48:40,315] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "[2019-06-14 06:48:40,356] {estimator.py:1113} INFO - Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
            "[2019-06-14 06:48:40,364] {export.py:587} INFO - Signatures INCLUDED in export for Predict: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "[2019-06-14 06:48:40,367] {export.py:587} INFO - Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "[2019-06-14 06:48:40,372] {export.py:587} INFO - Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
            "[2019-06-14 06:48:40,375] {export.py:587} INFO - Signatures INCLUDED in export for Eval: ['eval']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "[2019-06-14 06:48:40,382] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
            "WARNING:tensorflow:Export includes no default signature!\n",
            "[2019-06-14 06:48:40,390] {tf_logging.py:161} WARNING - Export includes no default signature!\n",
            "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "[2019-06-14 06:48:40,541] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "[2019-06-14 06:48:40,664] {builder_impl.py:654} INFO - Assets added to graph.\n",
            "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1560494917'/assets\n",
            "[2019-06-14 06:48:40,670] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1560494917'/assets\n",
            "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1560494917'/saved_model.pb\n",
            "[2019-06-14 06:48:41,018] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1560494917'/saved_model.pb\n",
            "INFO:tensorflow:Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
            "[2019-06-14 06:48:41,030] {executor.py:155} INFO - Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n"
          ]
        }
      ],
      "source": [
        "pipeline = DirectRunner().run(pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-l25zzKOKR"
      },
      "source": [
        "## Check Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aKyGMVIKOKR",
        "outputId": "e2e36510-bab2-487c-92cd-0e6952e19b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/taxi/data/simple/:\r\n",
            "total 1.9M\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:46 csv_example_gen\r\n",
            "1.9M -rw-r--r-- 1 root root 1.9M Jun 14 05:46 data.csv\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 schema_gen\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:46 statistics_gen\r\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Jun 14 06:47 trainer\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:46 transform\r\n",
            "\r\n",
            "/root/taxi/data/simple/csv_example_gen:\r\n",
            "total 8.0K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 eval\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 train\r\n",
            "\r\n",
            "/root/taxi/data/simple/csv_example_gen/eval:\r\n",
            "total 204K\r\n",
            "204K -rw-r--r-- 1 root root 201K Jun 14 06:46 data_tfrecord-00000-of-00001.gz\r\n",
            "\r\n",
            "/root/taxi/data/simple/csv_example_gen/train:\r\n",
            "total 408K\r\n",
            "408K -rw-r--r-- 1 root root 405K Jun 14 06:46 data_tfrecord-00000-of-00001.gz\r\n",
            "\r\n",
            "/root/taxi/data/simple/schema_gen:\r\n",
            "total 8.0K\r\n",
            "8.0K -rw-r--r-- 1 root root 4.5K Jun 14 06:46 schema.pbtxt\r\n",
            "\r\n",
            "/root/taxi/data/simple/statistics_gen:\r\n",
            "total 8.0K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 eval\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 train\r\n",
            "\r\n",
            "/root/taxi/data/simple/statistics_gen/eval:\r\n",
            "total 20K\r\n",
            "20K -rw-r--r-- 1 root root 17K Jun 14 06:46 stats_tfrecord\r\n",
            "\r\n",
            "/root/taxi/data/simple/statistics_gen/train:\r\n",
            "total 20K\r\n",
            "20K -rw-r--r-- 1 root root 18K Jun 14 06:46 stats_tfrecord\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer:\r\n",
            "total 4.0K\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:48 current\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current:\r\n",
            "total 8.0K\r\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Jun 14 06:48 eval_model_dir\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:48 serving_model_dir\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/eval_model_dir:\r\n",
            "total 4.0K\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:48 1560494917\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/eval_model_dir/1560494917:\r\n",
            "total 864K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:48 assets\r\n",
            "856K -rw-r--r-- 1 root root 854K Jun 14 06:48 saved_model.pb\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:48 variables\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/eval_model_dir/1560494917/assets:\r\n",
            "total 8.0K\r\n",
            "4.0K -rw-r--r-- 1 root root 1.3K Jun 14 06:48 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
            "4.0K -rw-r--r-- 1 root root   56 Jun 14 06:48 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/eval_model_dir/1560494917/variables:\r\n",
            "total 68K\r\n",
            "4.0K -rw-r--r-- 1 root root   8 Jun 14 06:48 variables.data-00000-of-00002\r\n",
            " 60K -rw-r--r-- 1 root root 58K Jun 14 06:48 variables.data-00001-of-00002\r\n",
            "4.0K -rw-r--r-- 1 root root 995 Jun 14 06:48 variables.index\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir:\r\n",
            "total 6.1M\r\n",
            "4.0K -rw-r--r-- 1 root root   89 Jun 14 06:48 checkpoint\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:47 eval_chicago-taxi-eval\r\n",
            "3.5M -rw-r--r-- 1 root root 3.5M Jun 14 06:48 events.out.tfevents.1560494830.646eb83be5d1\r\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Jun 14 06:48 export\r\n",
            "1.6M -rw-r--r-- 1 root root 1.6M Jun 14 06:47 graph.pbtxt\r\n",
            "4.0K -rw-r--r-- 1 root root    8 Jun 14 06:48 model.ckpt-10000.data-00000-of-00002\r\n",
            "128K -rw-r--r-- 1 root root 124K Jun 14 06:48 model.ckpt-10000.data-00001-of-00002\r\n",
            "4.0K -rw-r--r-- 1 root root 2.1K Jun 14 06:48 model.ckpt-10000.index\r\n",
            "824K -rw-r--r-- 1 root root 821K Jun 14 06:48 model.ckpt-10000.meta\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/eval_chicago-taxi-eval:\r\n",
            "total 1.3M\r\n",
            "1.3M -rw-r--r-- 1 root root 1.3M Jun 14 06:48 events.out.tfevents.1560494859.646eb83be5d1\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/export:\r\n",
            "total 4.0K\r\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Jun 14 06:48 chicago-taxi\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi:\r\n",
            "total 4.0K\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:48 1560494915\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915:\r\n",
            "total 564K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:48 assets\r\n",
            "556K -rw-r--r-- 1 root root 554K Jun 14 06:48 saved_model.pb\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:48 variables\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915/assets:\r\n",
            "total 8.0K\r\n",
            "4.0K -rw-r--r-- 1 root root 1.3K Jun 14 06:48 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
            "4.0K -rw-r--r-- 1 root root   56 Jun 14 06:48 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
            "\r\n",
            "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915/variables:\r\n",
            "total 68K\r\n",
            "4.0K -rw-r--r-- 1 root root   8 Jun 14 06:48 variables.data-00000-of-00002\r\n",
            " 60K -rw-r--r-- 1 root root 58K Jun 14 06:48 variables.data-00001-of-00002\r\n",
            "4.0K -rw-r--r-- 1 root root 995 Jun 14 06:48 variables.index\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform:\r\n",
            "total 8.0K\r\n",
            "4.0K drwxr-xr-x 5 root root 4.0K Jun 14 06:47 transform_output\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:47 transformed_examples\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output:\r\n",
            "total 12K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:47 metadata\r\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 06:46 transform_fn\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 transformed_metadata\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output/metadata:\r\n",
            "total 4.0K\r\n",
            "4.0K -rw-r--r-- 1 root root 916 Jun 14 06:47 schema.pbtxt\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output/transform_fn:\r\n",
            "total 84K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 assets\r\n",
            " 76K -rw-r--r-- 1 root root  76K Jun 14 06:46 saved_model.pb\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:46 variables\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output/transform_fn/assets:\r\n",
            "total 8.0K\r\n",
            "4.0K -rw-r--r-- 1 root root 1.3K Jun 14 06:46 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
            "4.0K -rw-r--r-- 1 root root   56 Jun 14 06:46 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output/transform_fn/variables:\r\n",
            "total 0\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transform_output/transformed_metadata:\r\n",
            "total 4.0K\r\n",
            "4.0K -rw-r--r-- 1 root root 2.2K Jun 14 06:46 schema.pbtxt\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transformed_examples:\r\n",
            "total 8.0K\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:47 eval\r\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 06:47 train\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transformed_examples/eval:\r\n",
            "total 176K\r\n",
            "176K -rw-r--r-- 1 root root 173K Jun 14 06:47 transformed_examples-00000-of-00001.gz\r\n",
            "\r\n",
            "/root/taxi/data/simple/transform/transformed_examples/train:\r\n",
            "total 348K\r\n",
            "348K -rw-r--r-- 1 root root 347K Jun 14 06:47 transformed_examples-00000-of-00001.gz\r\n"
          ]
        }
      ],
      "source": [
        "!ls -Rlhs /root/taxi/data/simple/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vk_L2rJKOKR",
        "outputId": "0d6db360-1dbb-49cb-c861-d6f85fba4dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915/variables/variables\n",
            "[2019-06-14 10:08:35,809] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915/variables/variables\n"
          ]
        }
      ],
      "source": [
        "with ops.Graph().as_default() as graph:\n",
        "    with tf.Session(graph=graph) as sess:\n",
        "        tf.saved_model.loader.load(sess,\n",
        "                                   [tf.saved_model.tag_constants.SERVING], \n",
        "                                   export_dir='/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915')\n",
        "        weights = graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1B4eMdJKOKR"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard.notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6xLMvyWKOKS",
        "outputId": "dad0aade-14d3-452b-92e5-cfcc334cc355"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600\"\n",
              "            src=\"http://localhost:6006\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f563bd83fd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1560494915"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNRyo8qyKOKS"
      },
      "source": [
        "`anomalies.pbtxt` is only simple text file. To visualize it, we are going to;\n",
        "\n",
        "1. get the path of `anomalies.pbtxt`from `example_validator`\n",
        "2. parse `anomalies.pbtxt` into anomalies (protobuf)\n",
        "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pgok40vKOKS"
      },
      "outputs": [],
      "source": [
        "# 1. get the path of `transformed_examples/train`\n",
        "def get_transformed_examples(transform):\n",
        "    artifacts = transform.outputs.transformed_examples.get()\n",
        "    return types.get_split_uri(artifacts, 'train')\n",
        "\n",
        "transformed_examples_dir = get_transformed_examples(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECMYV3h9KOKS"
      },
      "outputs": [],
      "source": [
        "# 2. generate statistics from tfrecord\n",
        "from pathlib import Path\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "def generate_stats(transformed_examples_dir):\n",
        "    path = Path(transformed_examples_dir)\n",
        "    for filepath in path.glob('*'):\n",
        "        # since we are using python 3.5, not 3.6+\n",
        "        return tfdv.generate_statistics_from_tfrecord(str(filepath))\n",
        "\n",
        "stats = generate_stats(transformed_examples_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEnv5BVfKOKT"
      },
      "outputs": [],
      "source": [
        "# 3. visualize statistics with tfdv\n",
        "\n",
        "tfdv.visualize_statistics(stats)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "TrainerTestRun.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}